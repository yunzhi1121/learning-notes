
### ✅ Slide 1

> Hi Professor, I’m Yunzhi Li.
> I’d like to walk you through what I’ve been working on recently.
> The topic is:
> **“Improving Node Feature Representation for Suspicious Transaction Detection.”**
> It’s mainly about improving how GNNs generate node embeddings in AML settings,
> with the goal of better capturing accounts or groups involved in suspicious activity.


### ✅ Slide 2：Motivation


> On this slide, I’d like to explain the main problem I’m focusing on.
>
> In models like GAGNN, node features are usually built by taking the **average of all past transactions**.
> But in reality, money laundering often happens in **short bursts** — for example, many small transfers or sudden activity within just a few days.
>
> When we mix recent and older transactions all together, those short-term spikes can **easily get lost**, and the model may miss them.
>
> So my idea is to **separate recent transactions from historical ones**, and use both when creating node features.
>
> This way, the model can understand what the account usually looks like, and also notice if anything **suddenly changes** in the recent period.


### ✅ Slide 3：My Design


> Now I’ll explain the method I designed.
>
> In the original GAGNN, node features are created by **averaging all past transactions**.
> But as I mentioned earlier, if suspicious activity happens suddenly, this average can **hide those recent changes**.
>
> So I propose a **dual-channel feature generator**.
>
> * One channel is **global** — it uses all historical transactions, just like the original method, to capture long-term behavior.
> * The other is **local** — it only looks at the **recent time window**, like the past 1 day, to focus on short-term anomalies.
>
> Then, instead of averaging again, I **concatenate** the two features, so that the model can use both global and local signals together.
>
> The key idea is that this keeps them **separate but complete**, and lets the model learn which part matters more in each case.


### ✅ 方法图详细讲解

> This diagram shows the overall workflow of my **dual-channel node feature generator**.
>
> There are two branches: the **global channel** at the top, and the **local (recent) channel** at the bottom.
>
> * On the left side, each circle represents an account, and the arrows indicate transaction relationships. The yellow node in the center is the account we are modeling.
>
> * In the **global channel**, I collect all historical transactions related to the target node. The features of those transactions form a matrix, which is then averaged to obtain a global feature vector (in green).
>   This reflects the **long-term behavior** of the account.
>
> * In the **local channel**, I select only the **most recent transactions** — for example, those within the past 1 day — and build a similar feature matrix. After aggregation, this gives the **recent behavior vector** (in orange), which highlights any sudden changes or bursts.
>
> * Each feature vector is then passed through a separate MLP (Multi-Layer Perceptron) to increase representation power.
>
> * Finally, I **concatenate** the two transformed feature vectors to form a single combined node embedding, which will be fed into the downstream prediction module.




### ✅ Slide 4：Technical Details


> This slide explains some key design choices in my method.
>
> First, a common question is:
> **Why not use weighted average or attention** to combine global and recent features?
>
> From what I’ve tested:
>
> * **Weighted average** blends the two signals too early, and that can cause confusion — the model can’t tell which part is from stable history, and which part is a recent anomaly.
> * **Attention** is flexible in theory, but it becomes unstable when a node has very few recent transactions. The attention weights then become noisy or meaningless.
>
> So, I find that using **concatenation** is a simpler and safer choice. It keeps both parts separate and lets the model learn how to use them.
>
> Second, how do I define “recent”?
> I currently use a **1-day window**, based on the data distribution and grid search.
> I also plan to compare different windows — like 3 days or 7 days — to see which one better captures suspicious behavior.
>
> Lastly, this feature module is added **before** the community detection part of GAGNN (that’s the GAT + eMRF module).
> So it doesn’t change the model structure — it just improves the quality of node features going in.



### ✅ Slide 5：Case Example


> Here I’d like to walk through a real example to show why we need **time-sensitive node features**.
>
> This is account **800AEAC00**, and we can see that on **September 8**, its activity suddenly spiked.
>
> On the left, we see the daily transaction count.
> Most days it made only 2 or 3 transactions, but on Sept. 8, it made **17** — that’s more than **3.6 times** the average of the previous days.
>
> On the right is its transaction subgraph — showing which accounts it interacted with.
> The **red edges** are marked as laundering transactions, and the **green ones** are normal.
> As we can see, most of the outgoing transfers on September 8 were suspicious — and the structure is kind of **star-shaped**, meaning it suddenly interacted with many new nodes.
>
> If we simply average the 10 days of activity, this spike would be flattened out, and the model might miss it completely.
>
> But with my method, the **local channel explicitly captures recent activity**, so bursts like this are more likely to be noticed by the model.



### ✅ Slide 6：Planned Experiment Setup


> Now I’d like to explain my current experiment setup.
>
> At this point, I’ve already reproduced the original GAGNN model, and I’m preparing to compare three different feature strategies:
>
> * The first is the **Baseline**, which uses only global average features from all past transactions.
> * The second uses **only recent transactions** within a fixed window — for example, one day — to build node features.
> * The third is **my method**, which concatenates both global and recent features using a dual-channel design.
>
> I’ll be using the **IBM Transactions for AML** dataset.
> Although it’s synthetic, it has clearly labeled suspicious activities and accounts, which makes it well-suited for evaluating AML detection models.
>
> For evaluation, I’ll focus on two metrics:
>
> * **Precision\@K**, especially for Top 1%, 5%, and 10%, to see how well the model identifies the most suspicious nodes.
> * And **AUC**, to measure the overall ranking quality of the model’s predictions.
>
> The key goal is to verify:
> **Does adding time-aware features actually improve the model’s ability to detect laundering behavior — especially group-based activity?**




### ✅ Slide 7：Conclusion & Next Steps

> I’d like to give a quick summary and outline the next steps.
>
> This work focuses on addressing a gap in existing AML GNN models —
> namely, their **lack of temporal sensitivity** when generating node features.
>
> I proposed a **dual-channel feature generator** that models long-term and recent behaviors separately, and combines them via concatenation.
> This avoids the information loss caused by averaging and makes the model more aware of **short-term spikes** that often indicate laundering.
>
> So far, I’ve completed the design, implemented the module, and reproduced the original GAGNN results.
> The next step is to run the full set of experiments.
>
> I’ll be using the IBM AML dataset to evaluate how different feature strategies perform in terms of Precision\@K and AUC.
> I also plan to test different time windows — such as 1-day, 3-day, and 7-day — to see how temporal granularity affects detection.
>
> Since there are **less than 3 weeks left** before the paper deadline, I’ll prioritize completing the ablation studies and key comparisons.
> If time allows, I’d like to try using attention **after feature extraction**, to let the model learn how to balance global and recent information — even though attention doesn’t work well on sparse transactions, it might still help at the fusion stage.
>
> Overall, my goal is to deliver a focused and well-motivated project with a clear temporal contribution.
> Thank you very much for your time and feedback!









