## 1. 概率基础

### 1.1. 高维随机变量的联合分布

> * **$P(x_1, x_2)$** ：这是**联合概率**（Joint Probability），表示随机变量 $X_1$ 取值 $x_1$ **且** $X_2$ 取值 $x_2$ 的概率，也就是
> 
>   $$
>   P(x_1, x_2) = \Pr\bigl(X_1 = x_1 \;\text{and}\; X_2 = x_2\bigr).
>   $$
> 
>   它告诉你这两个事件同时发生的可能性。
> 
> * **$P(x_2 \mid x_1)$** ：这是**条件概率**（Conditional Probability），表示在已知 $X_1 = x_1$ 的前提下，$X_2$ 取值 $x_2$ 的概率，也就是
> 
>   $$
>   P(x_2 \mid x_1) = \Pr\bigl(X_2 = x_2 \mid X_1 = x_1\bigr).
>   $$
> 
>   它反映了“当 $X_1$ 确定为 $x_1$ 后，$X_2$ 为 $x_2$ 的可能性”。
> 
> 两者之间的关系由**乘积规则**给出：
> 
> $$
> P(x_1, x_2) \;=\; P(x_1)\;P(x_2 \mid x_1)
> \quad\Longleftrightarrow\quad
> P(x_2 \mid x_1) \;=\;\frac{P(x_1, x_2)}{P(x_1)}.
> $$
> 
> * 用联合概率 $P(x_1,x_2)$ 可以计算任意两个变量一起发生的概率。
> * 用条件概率 $P(x_2\mid x_1)$ 可以在已知一个变量的情况下，推断另一个变量的分布。


对于 $p$ 个随机变量 $\{X_1,X_2,\dots,X_p\}$，它们的 **联合概率密度（或质量）** 写作

$$
P(X_1=x_1,\;X_2=x_2,\;\dots,\;X_p=x_p)\;=\;P(x_1,x_2,\dots,x_p).
$$

整个模型的核心，就是要对这个高维联合分布进行操作：

* **求边缘**（只关心部分变量）；
* **求条件**（已知一部分变量，推断另外一部分）；
* **分解联合**（把高维问题拆成一堆一维或低维的小问题）。

---

### 1.2. 边缘概率（Marginalization）——Sum Rule／积分规则

> **图中写法**：
>
> $$
> P(x_1)\;=\;\int P(x_1,x_2)\,\mathrm{d}x_2
> $$

或者对于离散情况：

$$
P(x_1)=\sum_{x_2}P(x_1,x_2).
$$

**含义**：从联合分布里“把 $X_2$ ∫ 掉”——只保留对 $X_1$ 的分布，即所谓“边缘化”。

* 用途：

  * 计算观测变量的总概率（evidence）。
  * 预测时“把不关心的潜变量”求和/积分掉。

---

### 1.3. 乘积规则（Product Rule）——Joint ↔ Conditional

> **图中写法**：
>
> $$
> P(x_1,x_2)\;=\;P(x_1)\,P(x_2\mid x_1)
> \;=\;P(x_2)\,P(x_1\mid x_2).
> $$

**推导**：由条件概率的定义

$$
P(x_2\mid x_1)=\frac{P(x_1,x_2)}{P(x_1)}
\quad\Longrightarrow\quad
P(x_1,x_2)=P(x_1)\,P(x_2\mid x_1).
$$

* 双向都能写：也可以 $P(x_2)P(x_1\mid x_2)$。
* **用途**：把“难以直接写出的联合”分解为“一个边缘×一个条件”，往往能引入判别模型或简化计算。

---

### 1.4. 链式规则（Chain Rule）——任意多维的分解

> **图中写法**：
>
> $$
> P(x_1,x_2,\dots,x_p)
> =\prod_{i=1}^p P\bigl(x_i\mid x_1,\dots,x_{i-1}\bigr).
> $$

**含义**：把 $p$ 维联合分布，拆成 $p$ 个一维条件分布的乘积。

* 第 1 项：$P(x_1)$；
* 第 2 项：$P(x_2\mid x_1)$； …；
* 第 $i$ 项：$P(x_i\mid x_1,\dots,x_{i-1})$。

**用途**：

* **贝叶斯网络**：在链式规则的基础上，进一步假设每个变量只与它的“父节点”有关（条件独立性），于是只要乘上那些局部 $P(x_i\mid\mathrm{pa}(x_i))$ 就够了，大大简化参数量。
* **变分推断、Gibbs 采样**：常用链式分解来设计更新步骤。

---

### 1.5. 在概率图模型中的应用

|         模型类型         | 因子分解形式                                                                        | 核心思想               |
| :------------------: | :---------------------------------------------------------------------------- | :----------------- |
|   贝叶斯网络 (Directed)   | $\displaystyle P(X_1,\dots,X_p)=\prod_i P\bigl(X_i\mid\mathrm{Pa}(X_i)\bigr)$ | 用有向图刻画条件独立性，基于链式规则 |
| 马尔可夫随机场 (Undirected) | $\displaystyle P(X)=\tfrac{1}{Z}\prod_{C\in\mathcal{C}}\psi_C(X_C)$           | 用无向图刻画马氏性，基于势函数分解  |

* **贝叶斯网**：

  * 拆分依据是“谁是谁的父节点”。
  * 只要知道每个节点 $X_i$ 在给定 $\mathrm{Pa}(X_i)$ 时的条件分布，就能组合成全局联合。
* **MRF／CRF**：

  * 拆分依据是图的团(clique)：给每个团一个**势函数**$\psi$。
  * 联合写作势函数乘积，再除以归一化常数 $Z$。

---

### 1.6. 小结

1. **Sum Rule**（积分/求和）让我们「边缘化」掉不关心的变量。
2. **Product Rule** 建立了联合分布与条件分布、边缘分布之间的桥梁。
3. **Chain Rule** 将任意高维联合分布分解成一系列一维或低维条件分布的乘积。
4. 在**贝叶斯网络**里，我们用链式规则＋条件独立假设做局部分解；在**MRF**里，则用势函数做无向分解。

掌握这三条基本法则，就能理解和推导各种概率图模型的因子分解与推断方法。

“高维随机变量”指的是由不止一个（通常很多个）随机变量组成的向量。例如，

$$
\mathbf X = (X_1, X_2, \dots, X_p)
$$

就是一个 $p$ 维的随机变量——我们说它“高维”，只是强调里面包含了多个分量 $X_i$。

---
## 2. 高维随机变量
### 2.1. 为什么要考虑高维随机变量？

1. **真实世界往往是多变量的**

   * 一张图像可以看做是成千上万个像素强度组成的高维变量；
   * 一份医学报告可能包含血压、心率、血糖等多项指标；
   * 金融资产组合里可能有上百只股票的收益率。
2. **联合分布才能捕捉变量间的依赖**

   * 如果只分别看每个变量的“边缘分布” $P(X_i)$，就丢失了变量之间的相关性；
   * 而高维联合分布

     $$
     P(X_1,\dots,X_p)
     $$

     则能完整刻画它们的相互影响和耦合。

---

### 2.2. 高维带来的计算挑战

* **维度灾难**：直接存储或学习一个 $p$ 维联合分布，参数量会随着维度呈指数级增长。
* **推断困难**：要做边缘化或条件推断，往往要对大量配置求和或积分，计算量巨大。

---

### 2.3. 概率图模型中的“解法”

概率图模型（PGM）——包括**贝叶斯网络**（有向）和**马尔可夫随机场**（无向）——正是为了解决高维联合分布不可管理而提出的。它们通过 **图结构＋局部分解** 来大幅降低复杂度：

| 模型      | 因子分解                                                                             | 直观含义                    |
| ------- | -------------------------------------------------------------------------------- | ----------------------- |
| 贝叶斯网络   | $\displaystyle P(\mathbf X)=\prod_{i=1}^p P\bigl(X_i\mid \mathrm{Pa}(X_i)\bigr)$ | 每个变量只依赖于它的“父节点”们        |
| 马尔可夫随机场 | $\displaystyle P(\mathbf X)=\frac1Z\prod_{C\in\mathcal C}\psi_C(X_C)$            | 只在小范围的“团”（Clique）里定义势函数 |

* **图节点** 对应单个随机变量 $X_i$。
* **图边**（或无向团）表示变量间的直接交互／依赖。
* **因子（factor）** 或 **势函数（potential）** 则是局部的低维函数，只涉及少数几个变量。

这样一来：

1. **参数减少**——只要分别学习那些小块因子的参数，而不必存 $O(n^p)$ 的表格；
2. **高效推断**——利用图结构可做可变消元（Variable Elimination）、信念传播（Belief Propagation）等算法，避免全局求和。

---

### 2.4. 举例：图像去噪中的高维处理

* 一幅 $100\times100$ 的灰度图像，可看作一个 $10^4$ 维随机向量。
* 马尔可夫随机场里我们通常只在“像素邻域”（比如上下左右四个邻居）之间连边，给每对邻居加一个平滑势函数。
* 因此，尽管整体是 $10^4$ 维，我们只需处理成千上万个“4 维”或“2 维”小问题，就能在可接受的时间里完成去噪或分割。

---

### 2.5. 小结

* **高维随机变量**：就是包含多个分量的随机向量，用以同时描述多项特征或观察值。
* **在概率图模型中的作用**：让我们用“图＋局部分解”的思路，针对高维联合分布做参数估计和推断——既保留变量间的相关性，又避免指数级的计算开销。


这三种马尔可夫性质——**成对（pairwise）**、**局部（local）**、**全局（global）**——其实都是对同一个无向图上随机变量条件独立关系的不同刻画，只是强度和形式各有侧重。关键在于：当联合分布是**正性**（positive，即所有配置概率都>0）时，这三条性质等价。下面把它们串起来跟刚才“观测阻断”的直观说法结合讲一遍。

---

## 1. 成对马尔可夫性（Pairwise Markov Property）

> **任意**图中**不相邻**的节点 $i,j$，在**给定其余所有节点** $\{X_{V\setminus\{i,j\}}\}$ 的值后，$X_i$ 与 $X_j$ 条件独立。

* 直观：如果两个节点之间连条边就表示它们“直接相互作用”，没边就表示“唯一的依赖路径里至少要经过别人”，于是给了“别人”之后，它们就隔断了。
* 对应“观测阻断”形式：把除了 $i$ 和 $j$ 外的所有节点都观测了，就没给这条唯一的链式或分叉／汇聚通路留下任何“空隙”。

---

## 2. 局部马尔可夫性（Local Markov Property）

> 每个节点 $i$，在**给定它所有邻居**（Markov blanket）$\mathrm{Nb}(i)$ 的值后，$X_i$ 与**其它非邻居**节点条件独立。

* 直观：只要把一个节点和它相连的那群直接伙伴（父子、兄弟……）都固定了，那么它就“和外面世界”断开了，不需要去看更远的节点。
* 这正是我们提到的“——当中间节点被观测，它就不再给更远端传递不确定性”的局部版。

---

## 3. 全局马尔可夫性（Global Markov Property）

> **任意**三组节点 $A,B,C$，只要图论上 $B$ **分离**了 $A$ 和 $C$（即所有 $A$→$C$ 路径都要过 $B$），就有

$$
X_A \;\perp\!\!\!\perp\; X_C \;\bigm|\; X_B.
$$

* 直观：这是最通用的“观测阻断”规则，不仅限于“一个节点”或“成对不邻”，而是任何两组节点只要被第三组挡在中间，就条件独立。
* 链式、分叉、汇聚，以及更复杂的多路径情况，都由它一网打尽。

---

## 三者的关系

| 性质       | 形式                  | 强弱关系             |
| -------- | ------------------- | ---------------- |
| Pairwise | 不邻节点在给定其余所有节点时独立    | 最弱，只关注成对         |
| Local    | 每个节点在给定邻居时与其它非邻居独立  | 中等，关注一个节点和它的“毯子” |
| Global   | 任意被分离的两组节点，给定分离集后独立 | 最强，覆盖所有情况        |

只要保证联合分布是**正性**的，这三条其实可以证明等价——给你图，你选任意一种说法，都能推出其它两种；也都等价于“势函数乘积形式”的因子分解。

---

### 小结

* “**观测阻断不确定性传递**”的直观比喻，就是全局马尔可夫性的精髓：任何时候，只要你把图上分离路径的节点都看到了，不确定性就断了。
* 局部／成对马尔可夫性，则是全局马尔可夫性的特例（只看一个节点或一对节点的情况），在正性假设下三者等价。这样，就能保证用**局部小块势函数**来组合出全局分布，既省参又可推断。
